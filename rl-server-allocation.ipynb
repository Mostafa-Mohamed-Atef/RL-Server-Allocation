{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-13T00:27:14.743074Z","iopub.execute_input":"2025-03-13T00:27:14.743524Z","iopub.status.idle":"2025-03-13T00:27:15.111389Z","shell.execute_reply.started":"2025-03-13T00:27:14.743474Z","shell.execute_reply":"2025-03-13T00:27:15.110418Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"##  Client-Server Environment Simulator","metadata":{}},{"cell_type":"code","source":"import gymnasium as gym","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T00:28:25.879569Z","iopub.execute_input":"2025-03-13T00:28:25.879943Z","iopub.status.idle":"2025-03-13T00:28:25.884569Z","shell.execute_reply.started":"2025-03-13T00:28:25.879913Z","shell.execute_reply":"2025-03-13T00:28:25.883409Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"class Server:\n    def __init__(self, capacity):\n        self.capacity = capacity  # Requests per second\n        self.active = False  # Whether the server is active","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T00:27:16.491784Z","iopub.execute_input":"2025-03-13T00:27:16.492260Z","iopub.status.idle":"2025-03-13T00:27:16.498026Z","shell.execute_reply.started":"2025-03-13T00:27:16.492223Z","shell.execute_reply":"2025-03-13T00:27:16.496924Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class ClientServerEnv:\n    def __init__(self, max_servers=10, demand_pattern='poisson'):\n        # Server settings\n        self.max_servers = max_servers\n        self.servers = [Server(capacity=10) for _ in range(max_servers)]\n        self.active_servers = 1  # Start with 1 server\n        \n        # Client demand\n        self.demand_pattern = demand_pattern  # 'poisson' or 'sinusoidal'\n        self.queue = []\n        \n        # RL settings\n        self.state_dim = 3  # [active_servers, queue_length, demand]\n        self.action_space = [-1, 0, +1]  # Actions: remove, keep, add\n        \n    def reset(self):\n        self.active_servers = 1\n        self.queue = []\n        return self._get_state()\n    \n    def _get_state(self):\n        # Normalized state: [active_servers, queue_length, demand]\n        return np.array([\n            self.active_servers / self.max_servers,\n            len(self.queue) / 100,  # Assume max queue=100\n            self._generate_demand() / 20  # Normalize demand\n        ])\n    \n    def _generate_demand(self):\n        if self.demand_pattern == 'poisson':\n            return np.random.poisson(lam=10)  # Avg 10 requests/sec\n        elif self.demand_pattern == 'sinusoidal':\n            return 10 + 10 * np.sin(self.time * 0.1)  # Time-varying\n    \n    def step(self, action):\n        # Update server count (clamp between 1 and max_servers)\n        self.active_servers = np.clip(\n            self.active_servers + action, 1, self.max_servers\n        )\n        \n        # Generate new requests\n        demand = self._generate_demand()\n        self.queue.extend([1] * int(demand))\n        \n        # Process requests\n        processed = min(\n            len(self.queue),\n            self.active_servers * self.servers[0].capacity  # All servers have same capacity\n        )\n        self.queue = self.queue[processed:]\n        \n        # Calculate reward\n        reward = self._calculate_reward(processed)\n        \n        # Next state\n        next_state = self._get_state()\n        done = False\n        return next_state, reward, done, {}\n    \n    def _calculate_reward(self, processed):\n        # Penalize under-provisioning (queue length)\n        queue_penalty = len(self.queue) * 0.1\n        # Penalize over-provisioning (idle servers)\n        idle_servers = self.active_servers - (processed / 10)\n        idle_penalty = idle_servers * 0.5\n        # Reward processed requests\n        processed_reward = processed * 1.0\n        return processed_reward - queue_penalty - idle_penalty","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T00:27:17.331299Z","iopub.execute_input":"2025-03-13T00:27:17.331648Z","iopub.status.idle":"2025-03-13T00:27:17.340737Z","shell.execute_reply.started":"2025-03-13T00:27:17.331600Z","shell.execute_reply":"2025-03-13T00:27:17.339651Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from stable_baselines3 import PPO\nenv = ClientServerEnv()\nmodel = PPO(\"MlpPolicy\", env, verbose=1)\nmodel.learn(total_timesteps=10_000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T00:28:33.706261Z","iopub.execute_input":"2025-03-13T00:28:33.706585Z","iopub.status.idle":"2025-03-13T00:28:33.735068Z","shell.execute_reply.started":"2025-03-13T00:28:33.706560Z","shell.execute_reply":"2025-03-13T00:28:33.733707Z"}},"outputs":[{"name":"stdout","text":"Using cpu device\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-d42b14014f83>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstable_baselines3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPPO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClientServerEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPPO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MlpPolicy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10_000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/ppo/ppo.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, policy, env, learning_rate, n_steps, batch_size, n_epochs, gamma, gae_lambda, clip_range, clip_range_vf, normalize_advantage, ent_coef, vf_coef, max_grad_norm, use_sde, sde_sample_freq, target_kl, stats_window_size, tensorboard_log, policy_kwargs, verbose, seed, device, _init_setup_model)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0m_init_setup_model\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     ):\n\u001b[0;32m--> 104\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    105\u001b[0m             \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, policy, env, learning_rate, n_steps, gamma, gae_lambda, ent_coef, vf_coef, max_grad_norm, use_sde, sde_sample_freq, stats_window_size, tensorboard_log, monitor_wrapper, policy_kwargs, verbose, seed, device, _init_setup_model, supported_action_spaces)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0msupported_action_spaces\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mType\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mspaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSpace\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     ):\n\u001b[0;32m---> 81\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0mpolicy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/base_class.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, policy, env, learning_rate, policy_kwargs, stats_window_size, tensorboard_log, verbose, device, support_multi_env, monitor_wrapper, seed, use_sde, sde_sample_freq, supported_action_spaces)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0menv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_make_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor_wrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/base_class.py\u001b[0m in \u001b[0;36m_wrap_env\u001b[0;34m(env, verbose, monitor_wrapper)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVecEnv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;31m# Patch to support gym 0.21/0.26 and gymnasium\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_patch_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMonitor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmonitor_wrapper\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/patch_gym.py\u001b[0m in \u001b[0;36m_patch_env\u001b[0;34m(env)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgym_installed\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEnv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0;34mf\"The environment is of type {type(env)}, not a Gymnasium \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;34mf\"environment. In this case, we expect OpenAI Gym to be \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: The environment is of type <class '__main__.ClientServerEnv'>, not a Gymnasium environment. In this case, we expect OpenAI Gym to be installed and the environment to be an OpenAI Gym environment."],"ename":"ValueError","evalue":"The environment is of type <class '__main__.ClientServerEnv'>, not a Gymnasium environment. In this case, we expect OpenAI Gym to be installed and the environment to be an OpenAI Gym environment.","output_type":"error"}],"execution_count":6},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nstates = []\nrewards = []\nobs = env.reset()\nfor _ in range(1000):\n    action, _ = model.predict(obs)\n    obs, reward, done, _ = env.step(action)\n    states.append(obs)\n    rewards.append(reward)\nplt.plot(rewards)\nplt.xlabel(\"Time Step\")\nplt.ylabel(\"Reward\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gymnasium as gym\nfrom gymnasium import spaces\nimport numpy as np\n\nclass ServerAllocationEnv(gym.Env):\n    metadata = {'render.modes': ['human']}\n    \n    def __init__(self, max_servers=10, demand_pattern='poisson'):\n        super(ServerAllocationEnv, self).__init__()\n        \n        # Server configuration\n        self.max_servers = max_servers\n        self.server_capacity = 10  # Requests/sec per server\n        self.active_servers = 1\n        \n        # Demand configuration\n        self.demand_pattern = demand_pattern\n        self.queue = []\n        self.time = 0\n        \n        # Gymnasium spaces\n        self.action_space = spaces.Discrete(3)  # 0=remove, 1=keep, 2=add\n        self.observation_space = spaces.Box(\n            low=np.array([0, 0, 0], dtype=np.float32),\n            high=np.array([1, 1, 1], dtype=np.float32),\n            shape=(3,), dtype=np.float32\n        )\n        \n    def reset(self, seed=None, options=None):\n        super().reset(seed=seed)\n        self.active_servers = 1\n        self.queue = []\n        self.time = 0\n        return self._get_state(), {}\n    \n    def _get_state(self):\n        return np.array([\n            self.active_servers / self.max_servers,\n            len(self.queue) / 100,  # Normalized queue length\n            self._generate_demand() / 20  # Normalized demand\n        ], dtype=np.float32)\n    \n    def _generate_demand(self):\n        if self.demand_pattern == 'poisson':\n            return self.np_random.poisson(lam=10)\n        elif self.demand_pattern == 'sinusoidal':\n            return 10 + 10 * np.sin(self.time * 0.1)\n        return 10\n    \n    def step(self, action):\n        # Convert action to server change (-1, 0, +1)\n        server_change = action - 1\n        self.active_servers = np.clip(\n            self.active_servers + server_change, 1, self.max_servers\n        )\n        \n        # Generate demand and process requests\n        demand = self._generate_demand()\n        self.queue.extend([1] * int(demand))\n        processed = min(len(self.queue), self.active_servers * self.server_capacity)\n        self.queue = self.queue[processed:]\n        \n        # Calculate reward\n        reward = self._calculate_reward(processed)\n        self.time += 1\n        \n        # Return Gymnasium step format\n        return self._get_state(), reward, False, False, {}\n    \n    def _calculate_reward(self, processed):\n        idle_servers = self.active_servers - (processed / self.server_capacity)\n        return (\n            processed * 1.0                # Reward for processed requests\n            - len(self.queue) * 0.1         # Penalize queue length\n            - idle_servers * 0.5            # Penalize idle servers\n        )\n    \n    def render(self, mode='human'):\n        print(f\"Time: {self.time} | Servers: {self.active_servers} | Queue: {len(self.queue)}\")\n    \n    def close(self):\n        pass\n\n# Example usage with Gymnasium\nif __name__ == \"__main__\":\n    env = ServerAllocationEnv()\n    obs, _ = env.reset()\n    \n    for _ in range(1000):\n        action = env.action_space.sample()  # Random policy\n        obs, reward, terminated, truncated, info = env.step(action)\n        \n        if _ % 100 == 0:\n            env.render()\n        \n        if terminated or truncated:\n            obs, _ = env.reset()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T00:30:50.038101Z","iopub.execute_input":"2025-03-13T00:30:50.038506Z","iopub.status.idle":"2025-03-13T00:30:50.086727Z","shell.execute_reply.started":"2025-03-13T00:30:50.038478Z","shell.execute_reply":"2025-03-13T00:30:50.085809Z"}},"outputs":[{"name":"stdout","text":"Time: 1 | Servers: 2 | Queue: 0\nTime: 101 | Servers: 4 | Queue: 0\nTime: 201 | Servers: 6 | Queue: 0\nTime: 301 | Servers: 4 | Queue: 0\nTime: 401 | Servers: 10 | Queue: 0\nTime: 501 | Servers: 10 | Queue: 0\nTime: 601 | Servers: 10 | Queue: 0\nTime: 701 | Servers: 4 | Queue: 0\nTime: 801 | Servers: 5 | Queue: 0\nTime: 901 | Servers: 8 | Queue: 0\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from stable_baselines3 import PPO\nenv = ServerAllocationEnv()\nmodel = PPO(\"MlpPolicy\", env, verbose=1)\nmodel.learn(total_timesteps=10_000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T00:31:04.646636Z","iopub.execute_input":"2025-03-13T00:31:04.647009Z","iopub.status.idle":"2025-03-13T00:31:22.091359Z","shell.execute_reply.started":"2025-03-13T00:31:04.646978Z","shell.execute_reply":"2025-03-13T00:31:22.090453Z"}},"outputs":[{"name":"stdout","text":"Using cpu device\nWrapping the env with a `Monitor` wrapper\nWrapping the env in a DummyVecEnv.\n-----------------------------\n| time/              |      |\n|    fps             | 1112 |\n|    iterations      | 1    |\n|    time_elapsed    | 1    |\n|    total_timesteps | 2048 |\n-----------------------------\n-----------------------------------------\n| time/                   |             |\n|    fps                  | 824         |\n|    iterations           | 2           |\n|    time_elapsed         | 4           |\n|    total_timesteps      | 4096        |\n| train/                  |             |\n|    approx_kl            | 0.012553623 |\n|    clip_fraction        | 0.188       |\n|    clip_range           | 0.2         |\n|    entropy_loss         | -1.09       |\n|    explained_variance   | -0.00283    |\n|    learning_rate        | 0.0003      |\n|    loss                 | 6.45e+03    |\n|    n_updates            | 10          |\n|    policy_gradient_loss | -0.0191     |\n|    value_loss           | 1.41e+04    |\n-----------------------------------------\n----------------------------------------\n| time/                   |            |\n|    fps                  | 771        |\n|    iterations           | 3          |\n|    time_elapsed         | 7          |\n|    total_timesteps      | 6144       |\n| train/                  |            |\n|    approx_kl            | 0.01270281 |\n|    clip_fraction        | 0.078      |\n|    clip_range           | 0.2        |\n|    entropy_loss         | -1.07      |\n|    explained_variance   | -0.0379    |\n|    learning_rate        | 0.0003     |\n|    loss                 | 8.03e+03   |\n|    n_updates            | 20         |\n|    policy_gradient_loss | -0.0162    |\n|    value_loss           | 1.84e+04   |\n----------------------------------------\n------------------------------------------\n| time/                   |              |\n|    fps                  | 743          |\n|    iterations           | 4            |\n|    time_elapsed         | 11           |\n|    total_timesteps      | 8192         |\n| train/                  |              |\n|    approx_kl            | 0.0025003306 |\n|    clip_fraction        | 0            |\n|    clip_range           | 0.2          |\n|    entropy_loss         | -1.06        |\n|    explained_variance   | -0.00224     |\n|    learning_rate        | 0.0003       |\n|    loss                 | 1.09e+04     |\n|    n_updates            | 30           |\n|    policy_gradient_loss | -0.00133     |\n|    value_loss           | 2.25e+04     |\n------------------------------------------\n-----------------------------------------\n| time/                   |             |\n|    fps                  | 727         |\n|    iterations           | 5           |\n|    time_elapsed         | 14          |\n|    total_timesteps      | 10240       |\n| train/                  |             |\n|    approx_kl            | 0.012271859 |\n|    clip_fraction        | 0.0145      |\n|    clip_range           | 0.2         |\n|    entropy_loss         | -1.02       |\n|    explained_variance   | -0.00185    |\n|    learning_rate        | 0.0003      |\n|    loss                 | 1.01e+04    |\n|    n_updates            | 40          |\n|    policy_gradient_loss | -0.00639    |\n|    value_loss           | 2.16e+04    |\n-----------------------------------------\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"<stable_baselines3.ppo.ppo.PPO at 0x7ab6448b1cc0>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}